{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGXocTFgBzlJUH1JhXBUSk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"KnQiHexVQZJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nnfs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czNnWVd7SFmC","executionInfo":{"status":"ok","timestamp":1738124463218,"user_tz":-330,"elapsed":3729,"user":{"displayName":"aaa kfaegl","userId":"01053767832317437010"}},"outputId":"04094c87-22b6-468d-9ee7-a917d966318d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nnfs\n","  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (1.26.4)\n","Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n","Installing collected packages: nnfs\n","Successfully installed nnfs-0.5.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import nnfs\n","from nnfs.datasets import spiral_data\n","\n","nnfs.init()\n","\n","class Layer_dense:\n","  def __init__(self, inputs, neurons):\n","    self.w = 0.01*np.random.randn(inputs, neurons)\n","    self.b = np.zeros((1, neurons))\n","\n","  def forward(self, inputs):\n","    self.output = np.dot(inputs, self.w) + self.b\n","\n","\n","class Relu:\n","  def forward(self, inputs):\n","    self.output = np.maximum(0, inputs)\n","\n","class Softmax:\n","  def forward(self, inputs):\n","    exp = np.exp(inputs - np.max(inputs, axis=1, keepdims =True))\n","\n","    probabilities = exp/np.sum(exp, axis= 1, keepdims=True)\n","    self.output = probabilities\n","\n","  def backward(self, dvalues):\n","    # Create uninitialized array\n","    self.dinputs = np.empty_like(dvalues)\n","    # Enumerate outputs and gradients\n","    for index, (single_output, single_dvalues) in \\\n","        enumerate(zip(self.output, dvalues)):\n","      # Flatten output array\n","      single_output = single_output.reshape(-1, 1)\n","      # Calculate Jacobian matrix of the output and\n","      jacobian_matrix = np.diagflat(single_output) - \\\n","                        np.dot(single_output, single_output.T)\n","      # Calculate sample-wise gradient\n","      # and add it to the array of sample gradients\n","      self.dinputs[index] = np.dot(jacobian_matrix,\n","                                   single_dvalues)\n","\n","\n","class Loss:\n","  def calculate(self, output, y):\n","      sample_loss = self.forward(output, y)\n","      data_loss = np.mean(sample_loss)\n","      return data_loss\n","#_______________________________________________________________________________\n","class Catagorical_cross_entropy(Loss):\n","  def forward(self, y_pred, y_true):   # y_pred -> predicted value\n","                                         # y_true -> actiual value\n","      samples = len(y_pred)\n","      y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n","\n","\n","# pribalility of targey value\n","      if len(y_true.shape)==1:\n","         correct_confidences = y_pred_clipped[range(samples), y_true]\n","\n","#     one-hor encoded lalbels\n","      elif len(y_true.shape)==2:\n","          correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n","# losses\n","      negative_log_likelihoods = -np.log(correct_confidences)\n","      return negative_log_likelihoods\n","\n","  def backward(self, dvalues, y_true):\n","\n","    samples = len(dvalues)\n","    labels = len(dvalues[0])  # Number of labels/classes\n","\n","    # If labels are sparse, turn them into one-hot vector\n","    if len(y_true.shape) == 1:\n","        y_true = np.eye(labels)[y_true]\n","\n","    # Calculate gradient\n","    self.dinputs = -y_true / dvalues\n","    # Normalize gradient\n","    self.dinputs = self.dinputs / samples\n","\n","\n","#_______________________________________________________________________________\n","#_______________________________________________________________________________\n","class Softmax_loss_categorical_crossentropy():\n","  def __init__(self):\n","    self.activation = Softmax()\n","    self.loss = Catagorical_cross_entropy()\n","\n","  def forward(self, inputs, y):\n","    self.activation.forward(inputs)\n","    self.outputs = self.activation.output # Changed from self.activation.outputs to self.activation.output\n","\n","    return self.loss.calculate(self.outputs, y)\n","\n","  def backward(self, dvalues, y):\n","    samples = len(dvalues)\n","\n","    if len(y.shape)==2:\n","      y = np.argmax(y, axis =1)\n","\n","    self.dinputs = dvalues.copy()\n","    self.dinputs[range(samples), y] -= 1\n","    self.dinputs = self.dinputs / samples"],"metadata":{"id":"qNMMf-fqZeCo","executionInfo":{"status":"ok","timestamp":1738127204641,"user_tz":-330,"elapsed":486,"user":{"displayName":"aaa kfaegl","userId":"01053767832317437010"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["y= np.array([[1, 0, 0],\n","             [0, 0, 1],\n","             [0, 1, 0]])\n","np.argmax(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q73S_kJ6QYEV","executionInfo":{"status":"ok","timestamp":1738124588092,"user_tz":-330,"elapsed":7,"user":{"displayName":"aaa kfaegl","userId":"01053767832317437010"}},"outputId":"50562f15-6cfe-493d-d361-9be2d59f30e0"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["print(np.argmax(y, axis = 0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TxOgpl5QuSd","executionInfo":{"status":"ok","timestamp":1738124590871,"user_tz":-330,"elapsed":676,"user":{"displayName":"aaa kfaegl","userId":"01053767832317437010"}},"outputId":"b83da955-65b1-428e-9754-a8a9257a830a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 2 1]\n"]}]},{"cell_type":"code","source":["softmax_outputs = np.array([[0.9, 0.8, 0.7],\n","                            [0.5, 0.4, 0.3],\n","                            [0.01, -0.7, -0.02]])\n","target = np.array([0, 1, 1])\n","\n","softmax_loss = Softmax_loss_categorical_crossentropy()\n","softmax_loss.backward(softmax_outputs, target)\n","dvalues1 = softmax_loss.dinputs\n","\n","activation = Softmax()\n","activation.output = softmax_outputs\n","\n","loss = Catagorical_cross_entropy()\n","loss.backward(softmax_outputs, target) # Now you can call backward on loss\n","activation.backward(loss.dinputs)\n","dvalues2 = activation.dinputs\n","\n","print(dvalues1)\n","print(dvalues2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFbAT0ksdCNp","executionInfo":{"status":"ok","timestamp":1738127191832,"user_tz":-330,"elapsed":479,"user":{"displayName":"aaa kfaegl","userId":"01053767832317437010"}},"outputId":"47c61c29-49d3-47f9-c21e-178abde8f6a2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.03333333  0.26666667  0.23333333]\n"," [ 0.16666667 -0.2         0.1       ]\n"," [ 0.00333333 -0.56666667 -0.00666667]]\n","[[-0.03333333  0.26666668  0.23333333]\n"," [ 0.16666667 -0.2         0.09999999]\n"," [ 0.00333333 -0.56666666 -0.00666667]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"j8--_7CDcpAS"},"execution_count":null,"outputs":[]}]}