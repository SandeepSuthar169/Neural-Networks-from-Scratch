{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6063,
     "status": "ok",
     "timestamp": 1737636298124,
     "user": {
      "displayName": "aaa kfaegl",
      "userId": "01053767832317437010"
     },
     "user_tz": -330
    },
    "id": "Gblv8LuaI4Zi",
    "outputId": "99a180aa-b724-4864-bf07-52cfb1c8836e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnfs\n",
      "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (1.26.4)\n",
      "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: nnfs\n",
      "Successfully installed nnfs-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1737638737477,
     "user": {
      "displayName": "aaa kfaegl",
      "userId": "01053767832317437010"
     },
     "user_tz": -330
    },
    "id": "DI65MjmYMXWv",
    "outputId": "e68f5172-955c-4d69-ef71-2320e7f7f365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1008677 0.3333333333333333\n",
      "1 1.0994315 0.3333333333333333\n",
      "2 1.0991219 0.3333333333333333\n",
      "3 1.0986339 0.3333333333333333\n",
      "4 1.0986199 0.3333333333333333\n",
      "5 1.0984716 0.36333333333333334\n",
      "6 1.0986457 0.33666666666666667\n",
      "7 1.1005241 0.35333333333333333\n",
      "8 1.1017133 0.3333333333333333\n",
      "9 1.099905 0.3333333333333333\n",
      "10 1.1005424 0.36\n",
      "11 1.1000216 0.3333333333333333\n",
      "12 1.0991852 0.3333333333333333\n",
      "13 1.0984135 0.32666666666666666\n",
      "14 1.10113 0.3333333333333333\n",
      "15 1.1001219 0.3333333333333333\n",
      "16 1.102658 0.3333333333333333\n",
      "17 1.1064461 0.3333333333333333\n",
      "18 1.1060681 0.3333333333333333\n",
      "19 1.1017231 0.3333333333333333\n",
      "20 1.1037397 0.3333333333333333\n",
      "21 1.1054286 0.3333333333333333\n",
      "22 1.1057986 0.3333333333333333\n",
      "23 1.1038558 0.3333333333333333\n",
      "24 1.1033031 0.3333333333333333\n",
      "25 1.1045598 0.3333333333333333\n",
      "26 1.1043073 0.35\n",
      "27 1.103653 0.32\n",
      "28 1.1047345 0.3466666666666667\n",
      "29 1.1061454 0.2733333333333333\n",
      "30 1.1062015 0.3233333333333333\n",
      "31 1.1076481 0.30666666666666664\n",
      "32 1.1135772 0.27666666666666667\n",
      "33 1.1090286 0.31\n",
      "34 1.1096791 0.3\n",
      "35 1.115957 0.28\n",
      "36 1.1150515 0.2833333333333333\n",
      "37 1.1166112 0.31333333333333335\n",
      "38 1.1169459 0.33666666666666667\n",
      "39 1.1195443 0.3333333333333333\n",
      "40 1.1270766 0.3333333333333333\n",
      "41 1.1303633 0.3333333333333333\n",
      "42 1.1301653 0.33666666666666667\n",
      "43 1.1313368 0.3333333333333333\n",
      "44 1.1278946 0.2966666666666667\n",
      "45 1.1407248 0.31\n",
      "46 1.1389267 0.31\n",
      "47 1.1352879 0.2966666666666667\n",
      "48 1.1414181 0.29333333333333333\n",
      "49 1.1584733 0.2866666666666667\n",
      "50 1.1534467 0.2833333333333333\n",
      "51 1.1478095 0.29\n",
      "52 1.1486379 0.30333333333333334\n",
      "53 1.1457034 0.32666666666666666\n",
      "54 1.1415874 0.3233333333333333\n",
      "55 1.1391742 0.29333333333333333\n",
      "56 1.1306242 0.30333333333333334\n",
      "57 1.1457554 0.3433333333333333\n",
      "58 1.1471267 0.32666666666666666\n",
      "59 1.1383275 0.2966666666666667\n",
      "60 1.1329619 0.2966666666666667\n",
      "61 1.1323985 0.2966666666666667\n",
      "62 1.1246194 0.3\n",
      "63 1.1164383 0.33666666666666667\n",
      "64 1.1156584 0.35\n",
      "65 1.1193972 0.32\n",
      "66 1.1206393 0.30333333333333334\n",
      "67 1.1210557 0.3233333333333333\n",
      "68 1.114367 0.35\n",
      "69 1.1181618 0.34\n",
      "70 1.1154691 0.3433333333333333\n",
      "71 1.1139673 0.3433333333333333\n",
      "72 1.112358 0.3433333333333333\n",
      "73 1.1100622 0.37\n",
      "74 1.1071788 0.35333333333333333\n",
      "75 1.1125846 0.3466666666666667\n",
      "76 1.1164114 0.34\n",
      "77 1.1201956 0.3433333333333333\n",
      "78 1.1187949 0.3333333333333333\n",
      "79 1.1181672 0.3233333333333333\n",
      "80 1.1177989 0.3333333333333333\n",
      "81 1.1165328 0.35\n",
      "82 1.1123487 0.3233333333333333\n",
      "83 1.1118795 0.34\n",
      "84 1.1126705 0.36333333333333334\n",
      "85 1.1116258 0.39\n",
      "86 1.112515 0.35\n",
      "87 1.1158819 0.33666666666666667\n",
      "88 1.1157863 0.33\n",
      "89 1.1168362 0.3466666666666667\n",
      "90 1.1308236 0.37666666666666665\n",
      "91 1.124866 0.39\n",
      "92 1.1261731 0.38666666666666666\n",
      "93 1.1264925 0.38\n",
      "94 1.1220986 0.35333333333333333\n",
      "95 1.1191846 0.32666666666666666\n",
      "96 1.1151248 0.37333333333333335\n",
      "97 1.1123258 0.37333333333333335\n",
      "98 1.1118472 0.37666666666666665\n",
      "99 1.1122351 0.37666666666666665\n",
      "99 1.1122351 0.37666666666666665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_dense:\n",
    "  def __init__(self, inputs, neurons):\n",
    "    self.w = 0.01*np.random.randn(inputs, neurons)\n",
    "    self.b = np.zeros((1, neurons))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.dot(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "class Relu:\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Softmax:\n",
    "  def forward(self, inputs):\n",
    "    exp = np.exp(inputs - np.max(inputs, axis=1, keepdims =True))\n",
    "\n",
    "    probabilities = exp/np.sum(exp, axis= 1, keepdims=True)\n",
    "    self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "  def calculate(self, output, y):\n",
    "      sample_loss = self.forward(output, y)\n",
    "      data_loss = np.mean(sample_loss)\n",
    "      return data_loss\n",
    "\n",
    "class Catagorical_cross_entropy(Loss):\n",
    "  def forward(self, y_pred, y_true):   # y_pred -> predicted value\n",
    "                                         # y_true -> actiual value\n",
    "      samples = len(y_pred)\n",
    "      y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "\n",
    "# pribalility of targey value\n",
    "      if len(y_true.shape)==1:\n",
    "         correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "#     one-hor encoded lalbels\n",
    "      elif len(y_true.shape)==2:\n",
    "          correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "# losses\n",
    "      negative_log_likelihoods = -np.log(correct_confidences)\n",
    "      return negative_log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "x, y =spiral_data(samples=100, classes=3)\n",
    "dense1 = Layer_dense(2, 3)\n",
    "activation1 = Relu()\n",
    "\n",
    "dense2 = Layer_dense(3, 3)\n",
    "activation2  = Softmax()\n",
    "\n",
    "loss_function = Catagorical_cross_entropy()\n",
    "\n",
    "lowest_loss = 99999999\n",
    "best_dense1_w = dense1.w.copy()\n",
    "best_dense1_b = dense1.b.copy()\n",
    "best_dense2_w = dense2.w.copy()\n",
    "best_dense2_b = dense2.b.copy()\n",
    "\n",
    "\n",
    "# update w with some small random values\n",
    "for iteration in range(100):\n",
    "  dense1.w += 0.05*np.random.randn(2,3)\n",
    "  dense1.b += 0.05*np.random.randn(1,3)\n",
    "\n",
    "  dense2.w +=0.05*np.random.randn(3,3)\n",
    "  dense2.b +=0.05*np.random.randn(1,3)\n",
    "\n",
    "#\n",
    "  dense1.forward(x)\n",
    "  activation1.forward(dense1.output)\n",
    "\n",
    "  dense2.forward(activation1.output)\n",
    "  activation2.forward(dense2.output)\n",
    "\n",
    "\n",
    "\n",
    "  loss = loss_function.calculate(activation2.output, y)\n",
    "  predictions = np.argmax(activation2.output, axis=1)\n",
    "  accuracy = np.mean(predictions==y)\n",
    "  print(iteration, loss, accuracy)\n",
    "\n",
    "\n",
    "\n",
    "if loss <lowest_loss:\n",
    "  print(iteration, loss, accuracy)\n",
    "  best_dense1_w = dense1.w.copy()\n",
    "  best_dense1_b = dense1.b.copy()\n",
    "  best_dense2_w = dense2.w.copy()\n",
    "  best_dense2_b = dense2.b.copy()\n",
    "  lowest_loss=loss\n",
    "\n",
    "else:\n",
    "  dense1.w = best_dense1_w.copy()\n",
    "  dense1.b = best_dense1_b.copy()\n",
    "  dense2.w = best_dense2_w.copy()\n",
    "  dense2.b = best_dense2_b.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYkVQwj1USgW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3DDcCw9K9IxRBVqHbOx12",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
